{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e09394",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff27f2",
   "metadata": {},
   "source": [
    "### 1. 安装gensim库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee44ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple/\n",
      "Requirement already satisfied: gensim in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages (from gensim) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebbcaaf",
   "metadata": {},
   "source": [
    "### 2. 同步数据至本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb2fc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "# mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/word_embedding/corpus.txt\", dst_url='corpus.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f02860",
   "metadata": {},
   "source": [
    "### 3. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6dccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13d3ff",
   "metadata": {},
   "source": [
    "### 4. 定义输入、输出文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4eab3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"corpus.txt\"\n",
    "out_embedding_file = \"embedding.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef20f35",
   "metadata": {},
   "source": [
    "### 5. 查看函数文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8009eca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mns_exponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhashfxn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msorted_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_final_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
       "\n",
       "Warnings\n",
       "--------\n",
       "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
       "such as lambda functions etc.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
       "\n",
       "Once you're finished training a model (=no more updates, only querying)\n",
       "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
       "to reduce memory.\n",
       "\n",
       "The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
       ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
       "\n",
       "The trained word vectors can also be stored/loaded from a format compatible with the\n",
       "original word2vec implementation via `self.wv.save_word2vec_format`\n",
       "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "sentences : iterable of iterables, optional\n",
       "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
       "    consider an iterable that streams the sentences directly from disk/network.\n",
       "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
       "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
       "    See also the `tutorial on data streaming in Python\n",
       "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
       "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
       "    in some other way.\n",
       "corpus_file : str, optional\n",
       "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
       "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
       "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
       "vector_size : int, optional\n",
       "    Dimensionality of the word vectors.\n",
       "window : int, optional\n",
       "    Maximum distance between the current and predicted word within a sentence.\n",
       "min_count : int, optional\n",
       "    Ignores all words with total frequency lower than this.\n",
       "workers : int, optional\n",
       "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
       "sg : {0, 1}, optional\n",
       "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
       "hs : {0, 1}, optional\n",
       "    If 1, hierarchical softmax will be used for model training.\n",
       "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
       "negative : int, optional\n",
       "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
       "    should be drawn (usually between 5-20).\n",
       "    If set to 0, no negative sampling is used.\n",
       "ns_exponent : float, optional\n",
       "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
       "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
       "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
       "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
       "    other values may perform better for recommendation applications.\n",
       "cbow_mean : {0, 1}, optional\n",
       "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
       "alpha : float, optional\n",
       "    The initial learning rate.\n",
       "min_alpha : float, optional\n",
       "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
       "seed : int, optional\n",
       "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
       "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
       "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
       "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
       "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
       "max_vocab_size : int, optional\n",
       "    Limits the RAM during vocabulary building; if there are more unique\n",
       "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
       "    Set to `None` for no limit.\n",
       "max_final_vocab : int, optional\n",
       "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
       "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
       "    Set to `None` if not required.\n",
       "sample : float, optional\n",
       "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
       "    useful range is (0, 1e-5).\n",
       "hashfxn : function, optional\n",
       "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
       "epochs : int, optional\n",
       "    Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
       "trim_rule : function, optional\n",
       "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
       "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
       "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
       "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
       "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
       "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
       "    model.\n",
       "\n",
       "    The input parameters are of the following types:\n",
       "        * `word` (str) - the word we are examining\n",
       "        * `count` (int) - the word's frequency count in the corpus\n",
       "        * `min_count` (int) - the minimum count threshold.\n",
       "sorted_vocab : {0, 1}, optional\n",
       "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
       "    See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
       "batch_words : int, optional\n",
       "    Target size (in words) for batches of examples passed to worker threads (and\n",
       "    thus cython routines).(Larger batches will be passed if individual\n",
       "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
       "compute_loss: bool, optional\n",
       "    If True, computes and stores loss value which can be retrieved using\n",
       "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
       "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
       "    Sequence of callbacks to be executed at specific stages during training.\n",
       "shrink_windows : bool, optional\n",
       "    New in 4.1. Experimental.\n",
       "    If True, the effective window size is uniformly sampled from  [1, `window`]\n",
       "    for each target word during training, to match the original word2vec algorithm's\n",
       "    approximate weighting of context words by distance. Otherwise, the effective\n",
       "    window size is always fixed to `window` words to either side.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.models import Word2Vec\n",
       "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
       "    >>> model = Word2Vec(sentences, min_count=1)\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
       "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
       "    directly to query those embeddings in various ways. See the module level docstring for examples.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/gensim/models/word2vec.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Doc2Vec, FastText\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea489334",
   "metadata": {},
   "source": [
    "### 6. 词向量训练并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1347a3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 434258 word types from a corpus of 8163235 raw words and 6566 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 99249 unique words (22.85% of original 434258, drops 335009)', 'datetime': '2023-04-20T16:51:26.830338', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 7661631 word corpus (93.86% of original 8163235, drops 501604)', 'datetime': '2023-04-20T16:51:26.830901', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 434258 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 19 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 7062927.526606419 word corpus (92.2%% of prior 7661631)', 'datetime': '2023-04-20T16:51:27.245472', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 99249 words and 100 dimensions: 129023700 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-20T16:51:28.038616', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 64 workers on 99249 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-04-20T16:51:28.039121', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 1.40% examples, 9315 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0 - PROGRESS: at 10.48% examples, 82560 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 8494704 raw words (6888198 effective words) took 11.3s, 607994 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 1.40% examples, 9403 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 12.24% examples, 93487 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 8494704 raw words (6889618 effective words) took 11.2s, 614168 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 1.40% examples, 9325 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 9.05% examples, 72948 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 8494704 raw words (6888907 effective words) took 11.3s, 612111 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 1.40% examples, 9205 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 16.27% examples, 120377 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 8494704 raw words (6888654 effective words) took 11.3s, 611737 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 1.40% examples, 9227 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 9.78% examples, 82172 words/s, in_qsize -1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 8494704 raw words (6889153 effective words) took 11.3s, 611594 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 42473520 raw words (34444530 effective words) took 57.2s, 601694 effective words/s', 'datetime': '2023-04-20T16:52:25.285408', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=99249, vector_size=100, alpha=0.025>', 'datetime': '2023-04-20T16:52:25.285940', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'created'}\n",
      "INFO:gensim.models.keyedvectors:storing 99249x100 projection weights into embedding.txt\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(corpus_file=corpus_file, vector_size=100, window=5, min_count=5, workers=cpu_count(), sg=1)\n",
    "model.wv.save_word2vec_format(out_embedding_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75fa1c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.txt  embedding.txt  word2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13683f",
   "metadata": {},
   "source": [
    "### 7. 加载离线词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d74cf611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from embedding.txt\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (99249, 100) matrix of type float32 from embedding.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-04-20T16:52:43.730113', 'gensim': '4.2.0', 'python': '3.7.10 | packaged by conda-forge | (default, Oct 13 2021, 21:01:18) \\n[GCC 9.4.0]', 'platform': 'Linux-4.18.0-147.5.1.6.h841.eulerosv2r9.x86_64-x86_64-with-debian-buster-sid', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"embedding.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512c51a",
   "metadata": {},
   "source": [
    "获取单个词的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0b0ac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.16473567e-02,  1.83549732e-01, -2.00295880e-01, -5.32540023e-01,\n",
       "       -3.00654382e-01,  1.91348702e-01,  3.75765353e-01,  4.96930122e-01,\n",
       "        2.49648720e-01, -4.69496995e-01,  5.84602803e-02, -4.77007657e-01,\n",
       "       -5.78321218e-01, -2.59597063e-01, -5.11817813e-01,  3.03647332e-02,\n",
       "        6.09854937e-01, -7.79630661e-01,  2.81163901e-02, -4.41169769e-01,\n",
       "        4.06336337e-01,  3.81375909e-01,  6.84741616e-01, -3.95616472e-01,\n",
       "       -4.02635574e-01,  3.06461304e-01, -3.99931461e-01, -4.01652545e-01,\n",
       "       -1.83616802e-02, -4.01387662e-01,  6.59308136e-02, -3.36004794e-02,\n",
       "       -5.64242415e-02, -4.16228443e-01, -3.35492671e-01,  1.65060535e-01,\n",
       "        4.64192063e-01,  7.26937711e-01, -2.21605539e-01,  3.64934534e-01,\n",
       "       -3.10870330e-03, -8.81859004e-01, -1.69672355e-01, -9.93212163e-02,\n",
       "       -2.87779197e-02,  7.55871227e-03, -4.52185690e-01,  1.41513348e-01,\n",
       "        3.37120771e-01,  2.27381483e-01, -1.04595885e-01,  6.53670311e-01,\n",
       "       -1.00690164e-01, -1.30476713e-01, -5.49758434e-01, -3.89094085e-01,\n",
       "        7.11358368e-01,  1.50794238e-01, -3.32081288e-01,  2.75963247e-02,\n",
       "       -3.21260095e-01, -1.00645706e-01,  3.11993994e-04, -9.97549891e-02,\n",
       "       -7.78676689e-01,  3.39935452e-01, -1.73609167e-01, -1.15732603e-01,\n",
       "        1.65060826e-03,  1.21354684e-01, -8.44213545e-01, -2.35318672e-02,\n",
       "        3.42834949e-01,  2.65681267e-01, -7.10580945e-01, -4.51325834e-01,\n",
       "        9.64165479e-02, -1.85544081e-02,  2.95785248e-01, -3.35960805e-01,\n",
       "        1.63458645e-01,  1.03724293e-01,  2.16644675e-01,  2.85863224e-02,\n",
       "       -1.43430114e-01, -7.95569420e-02,  3.83861214e-01,  1.02826662e-01,\n",
       "        4.61797982e-01, -3.76077741e-01,  3.93579364e-01,  2.07866967e-01,\n",
       "        2.93795288e-01,  5.78208268e-01,  4.60982502e-01, -3.66918236e-01,\n",
       "       -9.27951410e-02,  1.69665843e-01, -1.73952371e-01,  1.02992117e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model['中国']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803686ca",
   "metadata": {},
   "source": [
    "### 8. 相似度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8cf0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金融\n",
      "[('金融市场', 0.7948738932609558), ('金融服务', 0.7857430577278137), ('信贷', 0.7812432050704956), ('房地产', 0.7763000726699829), ('证券期货', 0.7721782326698303), ('银行学', 0.7701104283332825), ('投资银行', 0.7664597034454346), ('国际金融', 0.7633970379829407), ('金融学', 0.7576674222946167), ('期货交易', 0.7514584064483643)]\n",
      "喜欢\n",
      "[('讨厌', 0.7306100130081177), ('精力充沛', 0.6942640542984009), ('喝酒', 0.6932075023651123), ('小孩子', 0.6860583424568176), ('抽烟', 0.6853097677230835), ('打猎', 0.684668242931366), ('嗜好', 0.6827520132064819), ('胖', 0.6802889704704285), ('活该', 0.677295446395874), ('鲁特琴', 0.6770117282867432)]\n",
      "中国\n",
      "[('大陆', 0.7053304314613342), ('中华人民共和国', 0.5746998190879822), ('内地', 0.5635365843772888), ('外国', 0.5615857243537903), ('特步', 0.5512444376945496), ('沙排', 0.544475793838501), ('百富榜', 0.5443861484527588), ('中国地图出版社', 0.5403358340263367), ('中国邮政', 0.5370365977287292), ('宗教史', 0.5332518815994263)]\n",
      "北京\n",
      "[('天津', 0.7799407243728638), ('上海', 0.7443903684616089), ('南京', 0.7152845859527588), ('北京市', 0.6533621549606323), ('沈阳', 0.6457713842391968), ('西安', 0.6428720355033875), ('大连', 0.6416928768157959), ('联城', 0.6319024562835693), ('奥组委', 0.6245884299278259), ('深圳', 0.6226702928543091)]\n"
     ]
    }
   ],
   "source": [
    "testwords = ['金融', '喜欢', \"中国\", \"北京\"]\n",
    "for word in testwords:\n",
    "    res = word2vec_model.most_similar(word)\n",
    "    print (word)\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786319fc",
   "metadata": {},
   "source": [
    "### 9. 词向量文件回传obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32c8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "mox.file.copy_parallel(src_url=\"embedding.txt\", dst_url='obs://nlp-sjz/Word2vec/embedding.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18344d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
