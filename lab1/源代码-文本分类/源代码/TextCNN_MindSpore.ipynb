{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "# 配置文件设置\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,#情感只有正面负面两种所以设置最终分类为2类\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 4,#尝试修改使得存在warmup并没有改进\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'Ascend',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)#初始化运行设备，使用静态图模式，指定运行环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据预览\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:#查看前5条数据\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:#同上\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    '''\n",
    "    数据集的操作类\n",
    "    '''\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "        \n",
    "        #检查路径\n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)#建立字典对应one-hot编码\n",
    "        self.split_dataset(split=split)#划分训练集和测试集\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')#遍历分隔符号将句子划分为单词\n",
    "                sentence = list(filter(lambda x: x, sentence))#如果使用分词模块建立字典也许会更好\n",
    "                if sentence:#但是考虑到是英文以及评价词的词性一般不会变化其实影响不大\n",
    "                    self.word_num += len(sentence)#修改总的字典中单词数量\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)#更新单词的最大最小长度\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])#给数据打上正负标签制作数据集\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "\n",
    "        '''\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # self.Vocab['None']\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen#使用一个固定长度maxlen的向量表示该句子\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:#若该句子长度小于maxlen，则在后面不干什么，用0进行填充；\n",
    "                    break          #若该句子长度大于等于maxlen，则只选择前maxlen个单词作为文本信息输入\n",
    "                if word not in self.Vocab.keys():#如果这个位置的值是第一次出现，则将其加入到self.Vocab字典中\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1#并使用字典的长度-1作为该位置上的向量值；\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]#如果已经出现过，则直接使用其索引作为该位置上的向量值\n",
    "            SentenceLabel[0] = vector#对于每条句子，都会被转化为一个maxlen长度的向量，第二个维度是情感标签\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        分割为训练集与测试集\n",
    "\n",
    "        '''\n",
    "\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "        # random.shuffle(self.test)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获得数据集中文字组成的词典长度\n",
    "        '''\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.train))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.test))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)#读入数据划分数据集\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)#制作数据集\n",
    "batch_num = dataset.get_dataset_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[ 128,   15,   89 ...    0,    0,    0],\n",
      " [ 118,   15,  218 ...    0,    0,    0],\n",
      " [  15, 6705,   10 ...    0,    0,    0],\n",
      " ...\n",
      " [ 145,    2,   75 ...    0,    0,    0],\n",
      " [ 128, 2412,  128 ...    0,    0,    0],\n",
      " [   0,  747,  111 ...    0,    0,    0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, \n",
      " 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, \n",
      " 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0])}\n",
      "[118  15 218  11 219  88  32   0 100 220 221 122 222  86 223 224 225 226\n",
      " 227 228  82   2  36 229  36 230 231 155 232 164  63 155 233 234   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()#查看单词数以及数据集的情况\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []#设置lr，预热，在训练初期加速模型收敛；缩小学习率，训练后期缓解梯度爆炸现象，使得模型更加稳定和可靠。\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink#（迭代轮次比例按照for i中轮数确定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):#初始化卷积核权重生成\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):#制作卷积层，设置相关参数\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()#在输入张量中增加一个维度，使其变成四维，其作用是为了适配Conv2d网络层的输入格式。\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')#制作词向量，原因在报告中详细叙述，简单来说是因为卷积识别的特征是空间中的相关性。\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)#制作三种卷积层\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)#将第二个维度也就是预测标签拼接在一起\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)#全连接输出概率96*3是输入维度，也就是上述卷积层输出的维度\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)#防止过拟合\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)#求各个通道的均值操作\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),#激活函数\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),#最大值池化\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))#表示将经过求取均值后的多个特征图拼接起来，形成一个特征向量。\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)#生成自定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 1.6112439e-02 -2.0757150e-02 -2.0841148e-02 ... -1.8504603e-02\n",
      "        -2.8975620e-03 -2.7245784e-03]\n",
      "       [ 1.9803147e-03 -8.4856097e-03 -9.7931791e-03 ... -1.6013958e-02\n",
      "        -1.8243326e-03 -7.0100249e-04]\n",
      "       [ 5.8140219e-03 -1.3977945e-02 -6.3684967e-04 ... -7.4508167e-03\n",
      "         2.0056810e-02  2.3656346e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-8.3921067e-03  9.8413625e-04 -9.0313461e-03 ... -1.4636106e-02\n",
      "        -9.8245370e-04  4.8509962e-03]\n",
      "       [ 1.4972253e-03  2.8891333e-03 -1.3509471e-02 ... -1.8857339e-03\n",
      "         2.3570815e-02  1.5150110e-02]\n",
      "       [ 8.7881591e-03 -1.1051978e-02 -8.0734980e-04 ...  2.6710280e-03\n",
      "        -8.0229162e-04  1.8886685e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-3.9269803e-03  4.8205592e-03 -1.6872430e-03 ... -4.7693667e-03\n",
      "        -7.3476415e-03 -5.8992403e-03]\n",
      "       [-1.5458373e-02  6.0591646e-03 -1.2727292e-02 ... -8.0038840e-03\n",
      "         2.6894982e-03 -2.5496315e-02]\n",
      "       [ 9.7529781e-05  4.4836043e-03  1.4268911e-02 ... -3.6200194e-03\n",
      "         1.5876068e-02 -9.1702063e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 1.9832866e-03 -1.4666513e-02  4.1955584e-03 ... -5.3602736e-03\n",
      "         1.3194111e-03  6.4201391e-04]\n",
      "       [ 8.8883992e-03 -1.3355203e-03 -1.1793175e-02 ... -2.6757254e-03\n",
      "         1.4291412e-02 -1.0909875e-02]\n",
      "       [ 1.8606657e-02 -2.0644354e-02  1.6762996e-02 ...  9.5352548e-04\n",
      "         1.3557768e-02 -5.3202542e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.4664199e-03 -6.2415502e-03 -1.7630906e-03 ...  2.2789547e-03\n",
      "        -1.5540560e-03 -1.5069445e-02]\n",
      "       [-4.5781429e-03 -1.2054336e-02 -1.8174499e-02 ... -3.5096721e-03\n",
      "        -4.6958504e-03 -1.9499905e-03]\n",
      "       [-2.8188324e-03  1.4890456e-02  3.8987068e-03 ... -1.9592160e-02\n",
      "         8.9373123e-03  1.7731789e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-6.9340784e-03 -9.6059721e-03  1.0559484e-02 ...  2.0774366e-02\n",
      "        -2.3484461e-03  2.0511563e-06]\n",
      "       [-9.8140351e-03  7.3542719e-04  6.5863268e-03 ... -1.0168011e-02\n",
      "        -4.6939626e-03  4.3194410e-03]\n",
      "       [ 1.3129780e-02 -2.3458987e-02  1.0425770e-02 ...  5.4590427e-03\n",
      "         1.3071944e-02  3.6024952e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 0.00632537 -0.01063445 -0.00318262 ...  0.00501313 -0.00244617\n",
      "        -0.00613644]\n",
      "       [ 0.01681125 -0.00547193  0.00358948 ...  0.0055904  -0.01003921\n",
      "         0.00703614]\n",
      "       [-0.00687605  0.00828517  0.00594281 ...  0.00321279 -0.01241359\n",
      "        -0.00923256]\n",
      "       [ 0.00054306  0.00827809 -0.00852949 ...  0.01169318 -0.01874955\n",
      "         0.00181233]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00934273 -0.0048826  -0.00503131 ... -0.00727122 -0.01254461\n",
      "         0.00917111]\n",
      "       [-0.00755022  0.00595903  0.00863676 ... -0.00041442 -0.01176084\n",
      "         0.00124963]\n",
      "       [-0.02679469 -0.0018831   0.00912967 ... -0.00456609  0.01603748\n",
      "         0.00309811]\n",
      "       [-0.00622582  0.00284708  0.02045592 ... -0.00097225  0.00466994\n",
      "         0.00248718]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00543481  0.00822491  0.00701147 ... -0.00468088 -0.00995394\n",
      "        -0.00903249]\n",
      "       [-0.01099518  0.0069569   0.00885452 ...  0.00918005 -0.00752415\n",
      "        -0.00375624]\n",
      "       [-0.0019675  -0.00301035  0.00783609 ...  0.00091412  0.02019903\n",
      "         0.00408696]\n",
      "       [ 0.00148895  0.00204919 -0.01883048 ...  0.00061017 -0.01324234\n",
      "         0.02153736]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 0.00590541  0.01390271  0.00224864 ... -0.01721133  0.01730266\n",
      "        -0.00208624]\n",
      "       [-0.01637417  0.00921166 -0.0026152  ... -0.00683959  0.00303092\n",
      "         0.01205417]\n",
      "       [ 0.00489821  0.01180112 -0.00423777 ...  0.00472057 -0.00500233\n",
      "        -0.0130451 ]\n",
      "       [-0.00810326  0.00530614  0.00205408 ...  0.00564924  0.01293057\n",
      "        -0.00382231]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01868714  0.00387647  0.01106035 ... -0.01152015  0.01029707\n",
      "         0.01140455]\n",
      "       [ 0.00291103 -0.01345754 -0.00750393 ... -0.00743872 -0.0063428\n",
      "        -0.01703938]\n",
      "       [-0.01322555  0.01123757  0.01141789 ... -0.00672016  0.0066152\n",
      "        -0.00250123]\n",
      "       [-0.0137497  -0.00415406 -0.00818831 ...  0.00111848 -0.00135447\n",
      "         0.01194619]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.0165048  -0.01490281  0.01160467 ... -0.00742691  0.00202581\n",
      "         0.01924548]\n",
      "       [-0.00741627  0.01112317  0.00236558 ...  0.00364577 -0.01598345\n",
      "         0.01019782]\n",
      "       [ 0.00109056 -0.00225715  0.00459273 ... -0.0035871  -0.01780158\n",
      "        -0.00616355]\n",
      "       [ 0.00035566 -0.01762788  0.0116008  ... -0.02017879 -0.00029744\n",
      "        -0.01484381]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.25380438e-02  4.88902675e-03 -1.55318194e-04 ...  8.54769722e-03\n",
      "         1.13293529e-02  7.30303349e-03]\n",
      "       [ 9.36662033e-03  7.93810468e-03 -1.83968376e-02 ...  7.72806583e-03\n",
      "         6.56635454e-03  6.97452249e-03]\n",
      "       [ 9.00433306e-03  4.38018795e-03 -3.12009919e-03 ...  2.14600340e-02\n",
      "        -1.29224043e-02  3.92410997e-03]\n",
      "       [-6.75399881e-03 -8.39313772e-03  9.07563046e-03 ... -1.27264215e-02\n",
      "        -1.50959985e-02 -1.21036069e-02]\n",
      "       [ 4.52715252e-03  5.30229125e-04  7.13637285e-03 ... -6.11518091e-03\n",
      "         5.23411203e-03 -3.10184970e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-2.20505372e-02 -2.07484787e-04 -7.42193498e-03 ...  2.07399037e-02\n",
      "         9.81787872e-03 -1.85513683e-02]\n",
      "       [ 1.77605134e-02  9.77692474e-03  3.77408299e-03 ...  1.16298255e-02\n",
      "        -4.48276522e-03  9.09534562e-03]\n",
      "       [-5.78376837e-03  8.18144064e-03  2.17828219e-05 ...  2.40531657e-02\n",
      "        -2.48722779e-03  3.52952979e-03]\n",
      "       [-8.11538473e-03 -1.04682418e-02  9.08305310e-03 ...  7.26414728e-04\n",
      "        -1.84056014e-02  1.40055846e-02]\n",
      "       [ 2.34851474e-03 -1.28890071e-02  6.79625012e-03 ... -1.42299449e-02\n",
      "        -2.55923602e-03 -7.43998820e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-4.60888818e-03 -3.03049735e-03 -4.24831035e-03 ...  6.60028728e-03\n",
      "         1.71877053e-02 -8.90105031e-03]\n",
      "       [ 6.46607904e-03  4.15112032e-03 -6.07908098e-03 ...  6.02920400e-03\n",
      "        -8.12211260e-03 -7.28586048e-04]\n",
      "       [-5.53307543e-03 -4.20173211e-03 -8.79075564e-03 ... -7.62456853e-04\n",
      "        -1.47989579e-02 -1.03112096e-02]\n",
      "       [-1.57672651e-02  8.72490183e-03  6.30959542e-03 ...  5.17512090e-04\n",
      "         2.46352958e-03 -9.03745368e-03]\n",
      "       [-8.83167330e-03 -1.85227562e-02  1.03209633e-02 ... -1.42068826e-02\n",
      "         1.64449178e-02  1.09028518e-02]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 7.40306219e-03  8.55426583e-03 -8.19168333e-03 ... -6.97113574e-03\n",
      "        -7.87015632e-03 -8.35390668e-03]\n",
      "       [-1.34899886e-02 -7.61174317e-03  6.44844165e-03 ... -5.47810411e-03\n",
      "         1.35828890e-02  1.38532347e-03]\n",
      "       [ 5.88515867e-03  1.63168227e-03  1.40197261e-03 ...  9.89805814e-03\n",
      "        -3.98849137e-03  3.95161426e-03]\n",
      "       [-7.20545882e-03  3.95637238e-03  9.18118469e-03 ... -6.64692465e-03\n",
      "        -7.54398992e-03 -8.21562670e-03]\n",
      "       [ 5.83238341e-03 -1.08054755e-02 -3.02704517e-03 ... -5.12304343e-03\n",
      "        -7.09460117e-03 -1.58078708e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.19892263e-03  6.94240385e-04 -1.11934067e-04 ... -1.00284955e-02\n",
      "        -1.35923959e-02  2.08827411e-03]\n",
      "       [ 1.64754339e-03 -3.08188610e-04 -1.46312742e-02 ... -6.21467130e-04\n",
      "         5.19490917e-04 -7.82695785e-03]\n",
      "       [-1.03858893e-03  2.24385015e-03  6.74750889e-03 ... -1.22133335e-02\n",
      "        -3.26943304e-03  1.60733741e-02]\n",
      "       [-1.22246542e-03 -5.82158612e-03  4.57361806e-03 ...  3.73285380e-03\n",
      "        -6.47564745e-03 -1.00117410e-02]\n",
      "       [-7.09324749e-03  8.55631661e-03  5.75956935e-03 ...  2.67659989e-03\n",
      "        -2.98100081e-03 -4.17469675e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-3.39482841e-03 -3.85022489e-03  4.34952462e-03 ... -3.94331245e-03\n",
      "         1.10723032e-02 -5.71669126e-03]\n",
      "       [-3.00840312e-03 -7.72582320e-03 -1.35483057e-03 ... -6.71130884e-03\n",
      "         1.80799905e-02  5.88415843e-03]\n",
      "       [-5.54971397e-03  6.09158073e-03 -9.87017993e-03 ... -7.88795296e-03\n",
      "        -4.97747205e-05 -1.27120770e-03]\n",
      "       [ 3.77082336e-03 -6.62954245e-03  3.92297399e-04 ...  1.34932138e-02\n",
      "         2.39465758e-03  4.56306525e-03]\n",
      "       [ 1.29510812e-03  3.55761428e-03  9.06575006e-03 ...  1.03975590e-02\n",
      "         7.57795013e-03 -4.66349488e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "print(net)#打印网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training if set pre_trained to be True\n",
    "if cfg.pre_trained:#针对已经预训练好的网络，加载网络参数\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)#设置优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 596, loss is 0.09578466415405273\n",
      "epoch time: 17877.591 ms, per step time: 29.996 ms\n",
      "epoch: 2 step: 596, loss is 0.01619671657681465\n",
      "epoch time: 4872.288 ms, per step time: 8.175 ms\n",
      "epoch: 3 step: 596, loss is 0.0019015888683497906\n",
      "epoch time: 4792.765 ms, per step time: 8.042 ms\n",
      "epoch: 4 step: 596, loss is 0.003520732745528221\n",
      "epoch time: 4859.476 ms, per step time: 8.153 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn_1-5_745.ckpt'#调整epoch大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn_2-4_596.ckpt'#调整收缩阶段lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from [./ckpt/train_textcnn_2-4_596.ckpt].\n",
      "accuracy:  {'acc': 0.7666015625}\n"
     ]
    }
   ],
   "source": [
    "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=0.001, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n",
    "                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    param_dict = load_checkpoint(checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n",
    "else:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n",
    "\n",
    "load_param_into_net(net, param_dict)#根据checkpoint文件加载网络参数\n",
    "net.set_train(False)#将网络参数固定，防止验证时改动网络参数造成问题\n",
    "model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n",
    "\n",
    "acc = model.eval(dataset)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so boring\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "review_en = \"the movie is so happy\"\n",
    "inference(review_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
