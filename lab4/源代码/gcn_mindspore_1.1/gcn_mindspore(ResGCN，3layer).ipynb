{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MindSpore-GCN-论文分类\n",
    "### 1. 下载源码和数据至本地容器\n",
    "\n",
    "因为notebook是挂载在obs上，运行的容器实例不能直接读取操作obs上的文件，需下载至容器本地环境中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v1.17.3-d858ff4a\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/gcn_mindspore_1.1/data/\", dst_url='./data/')  # 将OBS桶中数据拷贝到容器中\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/gcn_mindspore_1.1/src/\", dst_url='./src/')\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/gcn_mindspore_1.1/graph_to_mindrecord/\", dst_url='./graph_to_mindrecord/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['DEVICE_ID']='7'\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.nn.layer.activation import get_activation\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from src.gcn import glorot, LossAccuracyWrapper, TrainNetWrapper\n",
    "from src.dataset import get_adj_features_labels, get_mask\n",
    "from graph_to_mindrecord.writer import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE,device_target=\"Ascend\", save_graphs=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'citeseer'\n",
    "datadir_save = './data_mr'\n",
    "datadir = os.path.join(datadir_save, dataname)\n",
    "cfg = edict({\n",
    "    'SRC_PATH': './data',\n",
    "    'MINDRECORD_PATH': datadir_save,\n",
    "    'DATASET_NAME': dataname,  # citeseer,cora\n",
    "    'mindrecord_partitions':1,\n",
    "    'mindrecord_header_size_by_bit' : 18,\n",
    "    'mindrecord_page_size_by_bit' : 20,\n",
    "\n",
    "    'data_dir': datadir,\n",
    "    'seed' : 123,\n",
    "    'train_nodes_num':140,#140\n",
    "    'eval_nodes_num':500,\n",
    "    'test_nodes_num':1000\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 转换数据格式为mindrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Graph To Mindrecord ==============\n",
      "Init writer  ...\n",
      "exec task 0, parallel: False ...\n",
      "Node task is 0\n",
      "transformed 512 record...\n",
      "transformed 1024 record...\n",
      "transformed 1536 record...\n",
      "transformed 2048 record...\n",
      "transformed 2560 record...\n",
      "transformed 3072 record...\n",
      "Processed 3312 lines for nodes.\n",
      "transformed 3312 record...\n",
      "exec task 0, parallel: False ...\n",
      "Edge task is 0\n",
      "Destination node 3309 does not exist.\n",
      "transformed 512 record...\n",
      "transformed 1024 record...\n",
      "transformed 1536 record...\n",
      "Destination node 3214 does not exist.\n",
      "transformed 2048 record...\n",
      "Destination node 3063 does not exist.\n",
      "Destination node 3063 does not exist.\n",
      "transformed 2560 record...\n",
      "transformed 3072 record...\n",
      "Destination node 2953 does not exist.\n",
      "Destination node 3042 does not exist.\n",
      "transformed 3584 record...\n",
      "Destination node 2553 does not exist.\n",
      "Destination node 3212 does not exist.\n",
      "transformed 4096 record...\n",
      "transformed 4608 record...\n",
      "Destination node 3305 does not exist.\n",
      "Destination node 3306 does not exist.\n",
      "transformed 5120 record...\n",
      "transformed 5632 record...\n",
      "Destination node 2781 does not exist.\n",
      "transformed 6144 record...\n",
      "Destination node 2682 does not exist.\n",
      "transformed 6656 record...\n",
      "Destination node 2407 does not exist.\n",
      "Source node 2407 does not exist.\n",
      "transformed 7168 record...\n",
      "Destination node 2489 does not exist.\n",
      "Source node 2489 does not exist.\n",
      "Source node 2553 does not exist.\n",
      "Destination node 3250 does not exist.\n",
      "Destination node 3292 does not exist.\n",
      "transformed 7680 record...\n",
      "Source node 2682 does not exist.\n",
      "Source node 2781 does not exist.\n",
      "transformed 8192 record...\n",
      "Destination node 3212 does not exist.\n",
      "Source node 2953 does not exist.\n",
      "transformed 8704 record...\n",
      "Source node 3042 does not exist.\n",
      "Source node 3063 does not exist.\n",
      "Source node 3063 does not exist.\n",
      "transformed 9216 record...\n",
      "Source node 3212 does not exist.\n",
      "Source node 3212 does not exist.\n",
      "Source node 3214 does not exist.\n",
      "Source node 3250 does not exist.\n",
      "Source node 3292 does not exist.\n",
      "Source node 3305 does not exist.\n",
      "Source node 3306 does not exist.\n",
      "Source node 3309 does not exist.\n",
      "Processed 9430 lines for edges.\n",
      "transformed 9430 record...\n",
      "--------------------------------------------\n",
      "END. Total time: 12.347697496414185\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 转换数据格式\n",
    "print(\"============== Graph To Mindrecord ==============\")\n",
    "run(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义GCN网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigGCN():\n",
    "    learning_rate = 0.01\n",
    "    epochs = 300\n",
    "    hidden1 = 16\n",
    "    hidden2 = 32\n",
    "    dropout = 0.5\n",
    "    weight_decay = 5e-4\n",
    "    early_stopping = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义GCN网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Cell):\n",
    "    \"\"\"\n",
    "    GCN graph convolution layer.\n",
    "\n",
    "    Args:\n",
    "        feature_in_dim (int): The input feature dimension.\n",
    "        feature_out_dim (int): The output feature dimension.\n",
    "        dropout_ratio (float): Dropout ratio for the dropout layer. Default: None.\n",
    "        activation (str): Activation function applied to the output of the layer, eg. 'relu'. Default: None.\n",
    "\n",
    "    Inputs:\n",
    "        - **adj** (Tensor) - Tensor of shape :math:`(N, N)`.\n",
    "        - **input_feature** (Tensor) - Tensor of shape :math:`(N, C)`.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 feature_in_dim,\n",
    "                 feature_out_dim,\n",
    "                 dropout_ratio=None,\n",
    "                 activation=None):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_dim = feature_in_dim\n",
    "        self.out_dim = feature_out_dim\n",
    "        self.weight_init = glorot([self.out_dim, self.in_dim])\n",
    "        self.fc = nn.Dense(self.in_dim,\n",
    "                           self.out_dim,\n",
    "                           weight_init=self.weight_init,\n",
    "                           has_bias=False)\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        if self.dropout_ratio is not None:\n",
    "            self.dropout = nn.Dropout(keep_prob=1-self.dropout_ratio)\n",
    "        self.dropout_flag = self.dropout_ratio is not None\n",
    "        self.activation = get_activation(activation)\n",
    "        self.activation_flag = self.activation is not None\n",
    "        self.matmul = P.MatMul()\n",
    "\n",
    "    def construct(self, adj, input_feature):\n",
    "        dropout = input_feature\n",
    "        if self.dropout_flag:\n",
    "            dropout = self.dropout(dropout)\n",
    "\n",
    "        fc = self.fc(dropout)\n",
    "        output_feature = self.matmul(adj, fc)\n",
    "\n",
    "        if self.activation_flag:\n",
    "            output_feature = self.activation(output_feature)\n",
    "        return output_feature\n",
    "\n",
    "\n",
    "class GCN(nn.Cell):\n",
    "    \"\"\"\n",
    "    GCN architecture.\n",
    "\n",
    "    Args:\n",
    "        config (ConfigGCN): Configuration for GCN.\n",
    "        adj (numpy.ndarray): Numbers of block in different layers.\n",
    "        feature (numpy.ndarray): Input channel in each layer.\n",
    "        output_dim (int): The number of output channels, equal to classes num.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, adj, feature, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.adj = Tensor(adj)\n",
    "        self.feature = Tensor(feature)\n",
    "        input_dim = feature.shape[1]\n",
    "        self.layer0 = GraphConvolution(input_dim, config.hidden1, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.layer1 = GraphConvolution(config.hidden1, config.hidden2, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.layer2 = GraphConvolution(config.hidden1, output_dim, dropout_ratio=None)\n",
    "\n",
    "    def construct(self):\n",
    "        output0 = self.layer0(self.adj, self.feature)\n",
    "        output1 = self.layer1(self.adj, output0)\n",
    "        output2 = self.layer2(self.adj, output1)\n",
    "        return output2\n",
    "    \n",
    "class ResGCN(nn.Cell):\n",
    "    \"\"\"\n",
    "    ResGCN architecture.\n",
    "\n",
    "    Args:\n",
    "        config (ConfigGCN): Configuration for GCN.\n",
    "        adj (numpy.ndarray): Numbers of block in different layers.\n",
    "        feature (numpy.ndarray): Input channel in each layer.\n",
    "        output_dim (int): The number of output channels, equal to classes num.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, adj, feature, output_dim):\n",
    "        super(ResGCN, self).__init__()\n",
    "        self.adj = Tensor(adj)\n",
    "        self.feature = Tensor(feature)\n",
    "        input_dim = feature.shape[1]\n",
    "        \n",
    "        # 3 layers of Graph Convolution with residual connections\n",
    "        self.layer0 = GraphConvolution(input_dim, config.hidden1, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.residual1 = nn.SequentialCell(\n",
    "            nn.Dense(input_dim, config.hidden1),\n",
    "            nn.BatchNorm1d(config.hidden1)\n",
    "        )\n",
    "        self.layer1 = GraphConvolution(config.hidden1, config.hidden2, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.residual2 = nn.SequentialCell(\n",
    "            nn.Dense(config.hidden1, config.hidden2),\n",
    "            nn.BatchNorm1d(config.hidden2)\n",
    "        )\n",
    "        self.layer2 = GraphConvolution(config.hidden2, output_dim, dropout_ratio=None)\n",
    "        \n",
    "    def construct(self):\n",
    "        out_res1 = self.residual1(self.feature) + self.layer0(self.adj, self.feature)\n",
    "        out_res2 = self.residual2(out_res1) + self.layer1(self.adj, out_res1)\n",
    "\n",
    "        output = self.layer2(self.adj, out_res2)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 定义训练、评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(args_opt):\n",
    "    \"\"\"Train model.\"\"\"\n",
    "    np.random.seed(args_opt.seed)\n",
    "    config = ConfigGCN()\n",
    "    adj, feature, label = get_adj_features_labels(args_opt.data_dir)\n",
    "\n",
    "    nodes_num = label.shape[0]\n",
    "    train_mask = get_mask(nodes_num, 0, args_opt.train_nodes_num)\n",
    "    eval_mask = get_mask(nodes_num, args_opt.train_nodes_num, args_opt.train_nodes_num + args_opt.eval_nodes_num)\n",
    "    test_mask = get_mask(nodes_num, nodes_num - args_opt.test_nodes_num, nodes_num)\n",
    "\n",
    "    class_num = label.shape[1]\n",
    "    ###############################################################change\n",
    "    gcn_net = GCN(config, adj, feature, class_num)\n",
    "    # gcn_net = ResGCN(config, adj, feature, class_num)\n",
    "    print(feature.shape[1])\n",
    "    # print(class_num)\n",
    "    gcn_net.add_flags_recursive(fp16=True)\n",
    "\n",
    "    eval_net = LossAccuracyWrapper(gcn_net, label, eval_mask, config.weight_decay)\n",
    "    test_net = LossAccuracyWrapper(gcn_net, label, test_mask, config.weight_decay)\n",
    "    train_net = TrainNetWrapper(gcn_net, label, train_mask, config)\n",
    "\n",
    "    loss_list = []\n",
    "    for epoch in range(config.epochs):\n",
    "        t = time.time()\n",
    "\n",
    "        train_net.set_train()\n",
    "        train_result = train_net()\n",
    "        train_loss = train_result[0].asnumpy()\n",
    "        train_accuracy = train_result[1].asnumpy()\n",
    "\n",
    "        eval_net.set_train(False)\n",
    "        eval_result = eval_net()\n",
    "        eval_loss = eval_result[0].asnumpy()\n",
    "        eval_accuracy = eval_result[1].asnumpy()\n",
    "\n",
    "        loss_list.append(eval_loss)\n",
    "        if epoch%10==0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch), \"train_loss=\", \"{:.5f}\".format(train_loss),\n",
    "                \"train_acc=\", \"{:.5f}\".format(train_accuracy), \"val_loss=\", \"{:.5f}\".format(eval_loss),\n",
    "                \"val_acc=\", \"{:.5f}\".format(eval_accuracy), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "        if epoch > config.early_stopping and loss_list[-1] > np.mean(loss_list[-(config.early_stopping+1):-1]):\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "    t_test = time.time()\n",
    "    test_net.set_train(False)\n",
    "    test_result = test_net()\n",
    "    test_loss = test_result[0].asnumpy()\n",
    "    test_accuracy = test_result[1].asnumpy()\n",
    "    print(\"Test set results:\", \"loss=\", \"{:.5f}\".format(test_loss),\n",
    "          \"accuracy=\", \"{:.5f}\".format(test_accuracy), \"time=\", \"{:.5f}\".format(time.time() - t_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 启动训练、评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "3703\n",
      "Epoch: 0000 train_loss= 1.79985 train_acc= 0.58571 val_loss= 1.79621 val_acc= 0.30600 time= 17.90567\n",
      "Epoch: 0010 train_loss= 1.75371 train_acc= 0.84286 val_loss= 1.77742 val_acc= 0.48000 time= 0.00459\n",
      "Epoch: 0020 train_loss= 1.70789 train_acc= 0.89286 val_loss= 1.76520 val_acc= 0.54800 time= 0.00455\n",
      "Epoch: 0030 train_loss= 1.66006 train_acc= 0.88571 val_loss= 1.75128 val_acc= 0.59000 time= 0.00467\n",
      "Epoch: 0040 train_loss= 1.59260 train_acc= 0.92143 val_loss= 1.73412 val_acc= 0.62000 time= 0.00455\n",
      "Epoch: 0050 train_loss= 1.53798 train_acc= 0.92143 val_loss= 1.71514 val_acc= 0.63000 time= 0.00454\n",
      "Epoch: 0060 train_loss= 1.46584 train_acc= 0.92857 val_loss= 1.69240 val_acc= 0.65600 time= 0.00443\n",
      "Epoch: 0070 train_loss= 1.37794 train_acc= 0.93571 val_loss= 1.67065 val_acc= 0.66400 time= 0.00455\n",
      "Epoch: 0080 train_loss= 1.32817 train_acc= 0.92857 val_loss= 1.64800 val_acc= 0.67400 time= 0.00455\n",
      "Epoch: 0090 train_loss= 1.23919 train_acc= 0.93571 val_loss= 1.62465 val_acc= 0.67800 time= 0.00449\n",
      "Epoch: 0100 train_loss= 1.19093 train_acc= 0.94286 val_loss= 1.60151 val_acc= 0.68400 time= 0.00458\n",
      "Epoch: 0110 train_loss= 1.11286 train_acc= 0.93571 val_loss= 1.57980 val_acc= 0.68800 time= 0.00450\n",
      "Epoch: 0120 train_loss= 1.08553 train_acc= 0.93571 val_loss= 1.55937 val_acc= 0.69400 time= 0.00457\n",
      "Epoch: 0130 train_loss= 1.03474 train_acc= 0.95000 val_loss= 1.53762 val_acc= 0.70000 time= 0.00449\n",
      "Epoch: 0140 train_loss= 0.99340 train_acc= 0.96429 val_loss= 1.51829 val_acc= 0.69400 time= 0.00456\n",
      "Epoch: 0150 train_loss= 0.95375 train_acc= 0.95000 val_loss= 1.49813 val_acc= 0.69800 time= 0.00450\n",
      "Epoch: 0160 train_loss= 0.92450 train_acc= 0.95000 val_loss= 1.48028 val_acc= 0.69600 time= 0.00447\n",
      "Epoch: 0170 train_loss= 0.89930 train_acc= 0.94286 val_loss= 1.46258 val_acc= 0.69600 time= 0.00467\n",
      "Epoch: 0180 train_loss= 0.85323 train_acc= 0.96429 val_loss= 1.44684 val_acc= 0.70000 time= 0.00452\n",
      "Epoch: 0190 train_loss= 0.85774 train_acc= 0.95000 val_loss= 1.43081 val_acc= 0.70400 time= 0.00453\n",
      "Epoch: 0200 train_loss= 0.80937 train_acc= 0.96429 val_loss= 1.41901 val_acc= 0.70000 time= 0.00450\n",
      "Epoch: 0210 train_loss= 0.78586 train_acc= 0.97143 val_loss= 1.40691 val_acc= 0.70000 time= 0.00484\n",
      "Epoch: 0220 train_loss= 0.79386 train_acc= 0.97857 val_loss= 1.39224 val_acc= 0.70000 time= 0.00457\n",
      "Epoch: 0230 train_loss= 0.75219 train_acc= 0.95714 val_loss= 1.38057 val_acc= 0.69800 time= 0.00447\n",
      "Epoch: 0240 train_loss= 0.72889 train_acc= 0.95714 val_loss= 1.37051 val_acc= 0.70000 time= 0.00450\n",
      "Epoch: 0250 train_loss= 0.72326 train_acc= 0.96429 val_loss= 1.36162 val_acc= 0.70000 time= 0.00450\n",
      "Epoch: 0260 train_loss= 0.73855 train_acc= 0.97143 val_loss= 1.34806 val_acc= 0.70600 time= 0.00448\n",
      "Epoch: 0270 train_loss= 0.70874 train_acc= 0.97857 val_loss= 1.33968 val_acc= 0.71000 time= 0.00448\n",
      "Epoch: 0280 train_loss= 0.67554 train_acc= 0.98571 val_loss= 1.33747 val_acc= 0.70000 time= 0.00448\n",
      "Epoch: 0290 train_loss= 0.67508 train_acc= 0.96429 val_loss= 1.32852 val_acc= 0.70200 time= 0.00452\n",
      "Test set results: loss= 1.29187 accuracy= 0.71900 time= 5.11539\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "print(\"============== Starting Training ==============\")\n",
    "train_eval(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
