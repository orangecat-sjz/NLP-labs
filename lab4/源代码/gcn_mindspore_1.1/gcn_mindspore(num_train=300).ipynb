{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MindSpore-GCN-论文分类\n",
    "### 1. 下载源码和数据至本地容器\n",
    "\n",
    "因为notebook是挂载在obs上，运行的容器实例不能直接读取操作obs上的文件，需下载至容器本地环境中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v1.17.3-d858ff4a\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/gcn_mindspore_1.1/data/\", dst_url='./data/')  # 将OBS桶中数据拷贝到容器中\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/gcn_mindspore_1.1/src/\", dst_url='./src/')\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/gcn_mindspore_1.1/graph_to_mindrecord/\", dst_url='./graph_to_mindrecord/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['DEVICE_ID']='7'\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from mindspore import nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.nn.layer.activation import get_activation\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from src.gcn import glorot, LossAccuracyWrapper, TrainNetWrapper\n",
    "from src.dataset import get_adj_features_labels, get_mask\n",
    "from graph_to_mindrecord.writer import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE,device_target=\"Ascend\", save_graphs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'cora'\n",
    "datadir_save = './data_mr'\n",
    "datadir = os.path.join(datadir_save, dataname)\n",
    "cfg = edict({\n",
    "    'SRC_PATH': './data',\n",
    "    'MINDRECORD_PATH': datadir_save,\n",
    "    'DATASET_NAME': dataname,  # citeseer,cora\n",
    "    'mindrecord_partitions':1,\n",
    "    'mindrecord_header_size_by_bit' : 18,\n",
    "    'mindrecord_page_size_by_bit' : 20,\n",
    "\n",
    "    'data_dir': datadir,\n",
    "    'seed' : 123,\n",
    "    'train_nodes_num':300,#140\n",
    "    'eval_nodes_num':500,\n",
    "    'test_nodes_num':1000\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 转换数据格式为mindrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Graph To Mindrecord ==============\n",
      "Init writer  ...\n",
      "exec task 0, parallel: False ...\n",
      "Node task is 0\n",
      "transformed 512 record...\n",
      "transformed 1024 record...\n",
      "transformed 1536 record...\n",
      "transformed 2048 record...\n",
      "transformed 2560 record...\n",
      "Processed 2708 lines for nodes.\n",
      "transformed 2708 record...\n",
      "exec task 0, parallel: False ...\n",
      "Edge task is 0\n",
      "transformed 512 record...\n",
      "transformed 1024 record...\n",
      "transformed 1536 record...\n",
      "transformed 2048 record...\n",
      "transformed 2560 record...\n",
      "transformed 3072 record...\n",
      "transformed 3584 record...\n",
      "transformed 4096 record...\n",
      "transformed 4608 record...\n",
      "transformed 5120 record...\n",
      "transformed 5632 record...\n",
      "transformed 6144 record...\n",
      "transformed 6656 record...\n",
      "transformed 7168 record...\n",
      "transformed 7680 record...\n",
      "transformed 8192 record...\n",
      "transformed 8704 record...\n",
      "transformed 9216 record...\n",
      "transformed 9728 record...\n",
      "transformed 10240 record...\n",
      "transformed 10752 record...\n",
      "Processed 10858 lines for edges.\n",
      "transformed 10858 record...\n",
      "--------------------------------------------\n",
      "END. Total time: 7.913086414337158\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 转换数据格式\n",
    "print(\"============== Graph To Mindrecord ==============\")\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义GCN网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigGCN():\n",
    "    learning_rate = 0.01\n",
    "    epochs = 300\n",
    "    hidden1 = 16\n",
    "    hidden2 = 32\n",
    "    dropout = 0.5\n",
    "    weight_decay = 5e-4\n",
    "    early_stopping = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义GCN网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Cell):\n",
    "    \"\"\"\n",
    "    GCN graph convolution layer.\n",
    "\n",
    "    Args:\n",
    "        feature_in_dim (int): The input feature dimension.\n",
    "        feature_out_dim (int): The output feature dimension.\n",
    "        dropout_ratio (float): Dropout ratio for the dropout layer. Default: None.\n",
    "        activation (str): Activation function applied to the output of the layer, eg. 'relu'. Default: None.\n",
    "\n",
    "    Inputs:\n",
    "        - **adj** (Tensor) - Tensor of shape :math:`(N, N)`.\n",
    "        - **input_feature** (Tensor) - Tensor of shape :math:`(N, C)`.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 feature_in_dim,\n",
    "                 feature_out_dim,\n",
    "                 dropout_ratio=None,\n",
    "                 activation=None):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_dim = feature_in_dim\n",
    "        self.out_dim = feature_out_dim\n",
    "        self.weight_init = glorot([self.out_dim, self.in_dim])\n",
    "        self.fc = nn.Dense(self.in_dim,\n",
    "                           self.out_dim,\n",
    "                           weight_init=self.weight_init,\n",
    "                           has_bias=False)\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        if self.dropout_ratio is not None:\n",
    "            self.dropout = nn.Dropout(keep_prob=1-self.dropout_ratio)\n",
    "        self.dropout_flag = self.dropout_ratio is not None\n",
    "        self.activation = get_activation(activation)\n",
    "        self.activation_flag = self.activation is not None\n",
    "        self.matmul = P.MatMul()\n",
    "\n",
    "    def construct(self, adj, input_feature):\n",
    "        dropout = input_feature\n",
    "        if self.dropout_flag:\n",
    "            dropout = self.dropout(dropout)\n",
    "\n",
    "        fc = self.fc(dropout)\n",
    "        output_feature = self.matmul(adj, fc)\n",
    "\n",
    "        if self.activation_flag:\n",
    "            output_feature = self.activation(output_feature)\n",
    "        return output_feature\n",
    "\n",
    "\n",
    "class GCN(nn.Cell):\n",
    "    \"\"\"\n",
    "    GCN architecture.\n",
    "\n",
    "    Args:\n",
    "        config (ConfigGCN): Configuration for GCN.\n",
    "        adj (numpy.ndarray): Numbers of block in different layers.\n",
    "        feature (numpy.ndarray): Input channel in each layer.\n",
    "        output_dim (int): The number of output channels, equal to classes num.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, adj, feature, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.adj = Tensor(adj)\n",
    "        self.feature = Tensor(feature)\n",
    "        input_dim = feature.shape[1]\n",
    "        self.layer0 = GraphConvolution(input_dim, config.hidden1, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        # self.layer1 = GraphConvolution(config.hidden1, config.hidden2, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.layer1 = GraphConvolution(config.hidden1, output_dim, dropout_ratio=None)\n",
    "\n",
    "    def construct(self):\n",
    "        output0 = self.layer0(self.adj, self.feature)\n",
    "        output1 = self.layer1(self.adj, output0)\n",
    "        # output2 = self.layer2(self.adj, output1)\n",
    "        return output1\n",
    "    \n",
    "class ResGCN(nn.Cell):\n",
    "    \"\"\"\n",
    "    ResGCN architecture.\n",
    "\n",
    "    Args:\n",
    "        config (ConfigGCN): Configuration for GCN.\n",
    "        adj (numpy.ndarray): Numbers of block in different layers.\n",
    "        feature (numpy.ndarray): Input channel in each layer.\n",
    "        output_dim (int): The number of output channels, equal to classes num.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, adj, feature, output_dim):\n",
    "        super(ResGCN, self).__init__()\n",
    "        self.adj = Tensor(adj)\n",
    "        self.feature = Tensor(feature)\n",
    "        input_dim = feature.shape[1]\n",
    "        \n",
    "        # 3 layers of Graph Convolution with residual connections\n",
    "        self.layer0 = GraphConvolution(input_dim, config.hidden1, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.residual1 = nn.SequentialCell(\n",
    "            nn.Dense(input_dim, config.hidden1),\n",
    "            nn.BatchNorm1d(config.hidden1)\n",
    "        )\n",
    "        self.layer1 = GraphConvolution(config.hidden1, config.hidden2, activation=\"relu\", dropout_ratio=config.dropout)\n",
    "        self.residual2 = nn.SequentialCell(\n",
    "            nn.Dense(config.hidden1, config.hidden2),\n",
    "            nn.BatchNorm1d(config.hidden2)\n",
    "        )\n",
    "        self.layer2 = GraphConvolution(config.hidden2, output_dim, dropout_ratio=None)\n",
    "        \n",
    "    def construct(self):\n",
    "        out_res1 = self.residual1(self.feature) + self.layer0(self.adj, self.feature)\n",
    "        out_res2 = self.residual2(out_res1) + self.layer1(self.adj, out_res1)\n",
    "\n",
    "        output = self.layer2(self.adj, out_res2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 定义训练、评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(args_opt):\n",
    "    \"\"\"Train model.\"\"\"\n",
    "    np.random.seed(args_opt.seed)\n",
    "    config = ConfigGCN()\n",
    "    adj, feature, label = get_adj_features_labels(args_opt.data_dir)\n",
    "\n",
    "    nodes_num = label.shape[0]\n",
    "    train_mask = get_mask(nodes_num, 0, args_opt.train_nodes_num)\n",
    "    eval_mask = get_mask(nodes_num, args_opt.train_nodes_num, args_opt.train_nodes_num + args_opt.eval_nodes_num)\n",
    "    test_mask = get_mask(nodes_num, nodes_num - args_opt.test_nodes_num, nodes_num)\n",
    "\n",
    "    class_num = label.shape[1]\n",
    "    gcn_net = GCN(config, adj, feature, class_num)\n",
    "    # gcn_net = ResGCN(config, adj, feature, class_num)\n",
    "    print(feature.shape[1])\n",
    "    # print(class_num)\n",
    "    gcn_net.add_flags_recursive(fp16=True)\n",
    "\n",
    "    eval_net = LossAccuracyWrapper(gcn_net, label, eval_mask, config.weight_decay)\n",
    "    test_net = LossAccuracyWrapper(gcn_net, label, test_mask, config.weight_decay)\n",
    "    train_net = TrainNetWrapper(gcn_net, label, train_mask, config)\n",
    "\n",
    "    loss_list = []\n",
    "    for epoch in range(config.epochs):\n",
    "        t = time.time()\n",
    "\n",
    "        train_net.set_train()\n",
    "        train_result = train_net()\n",
    "        train_loss = train_result[0].asnumpy()\n",
    "        train_accuracy = train_result[1].asnumpy()\n",
    "\n",
    "        eval_net.set_train(False)\n",
    "        eval_result = eval_net()\n",
    "        eval_loss = eval_result[0].asnumpy()\n",
    "        eval_accuracy = eval_result[1].asnumpy()\n",
    "\n",
    "        loss_list.append(eval_loss)\n",
    "        if epoch%10==0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch), \"train_loss=\", \"{:.5f}\".format(train_loss),\n",
    "                \"train_acc=\", \"{:.5f}\".format(train_accuracy), \"val_loss=\", \"{:.5f}\".format(eval_loss),\n",
    "                \"val_acc=\", \"{:.5f}\".format(eval_accuracy), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "        if epoch > config.early_stopping and loss_list[-1] > np.mean(loss_list[-(config.early_stopping+1):-1]):\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "    t_test = time.time()\n",
    "    test_net.set_train(False)\n",
    "    test_result = test_net()\n",
    "    test_loss = test_result[0].asnumpy()\n",
    "    test_accuracy = test_result[1].asnumpy()\n",
    "    print(\"Test set results:\", \"loss=\", \"{:.5f}\".format(test_loss),\n",
    "          \"accuracy=\", \"{:.5f}\".format(test_accuracy), \"time=\", \"{:.5f}\".format(time.time() - t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 启动训练、评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "1433\n",
      "Epoch: 0000 train_loss= 1.95391 train_acc= 0.54000 val_loss= 1.94732 val_acc= 0.33200 time= 12.31320\n",
      "Epoch: 0010 train_loss= 1.86925 train_acc= 0.73000 val_loss= 1.88418 val_acc= 0.58800 time= 0.00391\n",
      "Epoch: 0020 train_loss= 1.76162 train_acc= 0.77667 val_loss= 1.81086 val_acc= 0.62400 time= 0.00408\n",
      "Epoch: 0030 train_loss= 1.63402 train_acc= 0.77667 val_loss= 1.72245 val_acc= 0.66400 time= 0.00400\n",
      "Epoch: 0040 train_loss= 1.49783 train_acc= 0.82333 val_loss= 1.62256 val_acc= 0.70000 time= 0.00401\n",
      "Epoch: 0050 train_loss= 1.34805 train_acc= 0.84000 val_loss= 1.52231 val_acc= 0.73000 time= 0.00388\n",
      "Epoch: 0060 train_loss= 1.23188 train_acc= 0.89667 val_loss= 1.42798 val_acc= 0.77600 time= 0.00386\n",
      "Epoch: 0070 train_loss= 1.12039 train_acc= 0.90333 val_loss= 1.34572 val_acc= 0.79200 time= 0.00408\n",
      "Epoch: 0080 train_loss= 1.00142 train_acc= 0.94333 val_loss= 1.27430 val_acc= 0.80200 time= 0.00411\n",
      "Epoch: 0090 train_loss= 0.94228 train_acc= 0.94333 val_loss= 1.21656 val_acc= 0.81400 time= 0.00424\n",
      "Epoch: 0100 train_loss= 0.88648 train_acc= 0.94667 val_loss= 1.16448 val_acc= 0.81200 time= 0.00404\n",
      "Epoch: 0110 train_loss= 0.82863 train_acc= 0.95667 val_loss= 1.12569 val_acc= 0.82800 time= 0.00399\n",
      "Epoch: 0120 train_loss= 0.78305 train_acc= 0.95333 val_loss= 1.08829 val_acc= 0.82800 time= 0.00407\n",
      "Epoch: 0130 train_loss= 0.73092 train_acc= 0.96333 val_loss= 1.05607 val_acc= 0.82600 time= 0.00404\n",
      "Epoch: 0140 train_loss= 0.72086 train_acc= 0.96333 val_loss= 1.02966 val_acc= 0.83000 time= 0.00419\n",
      "Epoch: 0150 train_loss= 0.70037 train_acc= 0.94667 val_loss= 1.00833 val_acc= 0.82800 time= 0.00396\n",
      "Epoch: 0160 train_loss= 0.65456 train_acc= 0.97667 val_loss= 0.99019 val_acc= 0.82800 time= 0.00416\n",
      "Epoch: 0170 train_loss= 0.64604 train_acc= 0.97333 val_loss= 0.97301 val_acc= 0.83400 time= 0.00412\n",
      "Epoch: 0180 train_loss= 0.62397 train_acc= 0.97333 val_loss= 0.95899 val_acc= 0.82800 time= 0.00389\n",
      "Epoch: 0190 train_loss= 0.60689 train_acc= 0.95667 val_loss= 0.94245 val_acc= 0.82000 time= 0.00383\n",
      "Epoch: 0200 train_loss= 0.59210 train_acc= 0.98000 val_loss= 0.93001 val_acc= 0.82200 time= 0.00402\n",
      "Epoch: 0210 train_loss= 0.56849 train_acc= 0.98333 val_loss= 0.92146 val_acc= 0.82600 time= 0.00387\n",
      "Epoch: 0220 train_loss= 0.55549 train_acc= 0.97333 val_loss= 0.90883 val_acc= 0.82200 time= 0.00388\n",
      "Epoch: 0230 train_loss= 0.56235 train_acc= 0.97667 val_loss= 0.89365 val_acc= 0.81800 time= 0.00407\n",
      "Epoch: 0240 train_loss= 0.53760 train_acc= 0.98333 val_loss= 0.88803 val_acc= 0.82000 time= 0.00391\n",
      "Epoch: 0250 train_loss= 0.52225 train_acc= 0.97333 val_loss= 0.87791 val_acc= 0.81600 time= 0.00391\n",
      "Epoch: 0260 train_loss= 0.49274 train_acc= 0.99333 val_loss= 0.87068 val_acc= 0.81800 time= 0.00373\n",
      "Epoch: 0270 train_loss= 0.52290 train_acc= 0.97667 val_loss= 0.86454 val_acc= 0.80800 time= 0.00430\n",
      "Epoch: 0280 train_loss= 0.48748 train_acc= 0.98333 val_loss= 0.85708 val_acc= 0.82200 time= 0.00398\n",
      "Epoch: 0290 train_loss= 0.48616 train_acc= 0.99000 val_loss= 0.85201 val_acc= 0.82200 time= 0.00410\n",
      "Test set results: loss= 0.77941 accuracy= 0.85100 time= 4.84083\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "print(\"============== Starting Training ==============\")\n",
    "train_eval(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
